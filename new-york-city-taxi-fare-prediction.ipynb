{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:38.165107Z","iopub.status.busy":"2022-12-01T16:46:38.164047Z","iopub.status.idle":"2022-12-01T16:46:38.188023Z","shell.execute_reply":"2022-12-01T16:46:38.18611Z","shell.execute_reply.started":"2022-12-01T16:46:38.164923Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:38.19172Z","iopub.status.busy":"2022-12-01T16:46:38.1907Z","iopub.status.idle":"2022-12-01T16:46:39.111052Z","shell.execute_reply":"2022-12-01T16:46:39.109251Z","shell.execute_reply.started":"2022-12-01T16:46:38.191668Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np \n","import datetime as dt\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge, Lasso\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor"]},{"cell_type":"markdown","metadata":{},"source":["## **<span style = 'color:green'>3. Load the dataset</span>**<a id =\"Data\"></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.113849Z","iopub.status.busy":"2022-12-01T16:46:39.11324Z","iopub.status.idle":"2022-12-01T16:46:39.246679Z","shell.execute_reply":"2022-12-01T16:46:39.24564Z","shell.execute_reply.started":"2022-12-01T16:46:39.113797Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=25000)\n","#df= pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', skiprows=lambda x: x%2 == 1)\n","df_test = pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\n","df.head()\n","df_cols = ['unique_id', 'amount', 'date_time_of_pickup', 'longitude_of_pickup', 'latitude_of_pickup', 'longitude_of_dropoff','latitude_of_dropoff', 'no_of_passenger']\n","df.columns = ufo_cols\n","# see modified columnsÂ \n","df.column"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.250292Z","iopub.status.busy":"2022-12-01T16:46:39.249228Z","iopub.status.idle":"2022-12-01T16:46:39.260678Z","shell.execute_reply":"2022-12-01T16:46:39.259216Z","shell.execute_reply.started":"2022-12-01T16:46:39.250241Z"},"trusted":true},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.264015Z","iopub.status.busy":"2022-12-01T16:46:39.262463Z","iopub.status.idle":"2022-12-01T16:46:39.287748Z","shell.execute_reply":"2022-12-01T16:46:39.285597Z","shell.execute_reply.started":"2022-12-01T16:46:39.263943Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.290387Z","iopub.status.busy":"2022-12-01T16:46:39.28986Z","iopub.status.idle":"2022-12-01T16:46:39.339258Z","shell.execute_reply":"2022-12-01T16:46:39.337581Z","shell.execute_reply.started":"2022-12-01T16:46:39.290338Z"},"trusted":true},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Checking for Missing Values</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.341771Z","iopub.status.busy":"2022-12-01T16:46:39.341215Z","iopub.status.idle":"2022-12-01T16:46:39.359417Z","shell.execute_reply":"2022-12-01T16:46:39.357921Z","shell.execute_reply.started":"2022-12-01T16:46:39.341721Z"},"trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.362222Z","iopub.status.busy":"2022-12-01T16:46:39.361368Z","iopub.status.idle":"2022-12-01T16:46:39.376867Z","shell.execute_reply":"2022-12-01T16:46:39.375362Z","shell.execute_reply.started":"2022-12-01T16:46:39.362169Z"},"trusted":true},"outputs":[],"source":["df_test.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.385195Z","iopub.status.busy":"2022-12-01T16:46:39.384122Z","iopub.status.idle":"2022-12-01T16:46:39.420823Z","shell.execute_reply":"2022-12-01T16:46:39.419463Z","shell.execute_reply.started":"2022-12-01T16:46:39.385122Z"},"trusted":true},"outputs":[],"source":["df.nunique()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:brown;\">Checking for duplicates</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.424336Z","iopub.status.busy":"2022-12-01T16:46:39.423226Z","iopub.status.idle":"2022-12-01T16:46:39.468072Z","shell.execute_reply":"2022-12-01T16:46:39.46709Z","shell.execute_reply.started":"2022-12-01T16:46:39.424277Z"},"trusted":true},"outputs":[],"source":["df.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.470075Z","iopub.status.busy":"2022-12-01T16:46:39.469689Z","iopub.status.idle":"2022-12-01T16:46:39.500031Z","shell.execute_reply":"2022-12-01T16:46:39.498412Z","shell.execute_reply.started":"2022-12-01T16:46:39.470041Z"},"trusted":true},"outputs":[],"source":["df.dropna(axis=0, inplace=True)\n","np.sum(pd.isnull(df))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.502425Z","iopub.status.busy":"2022-12-01T16:46:39.502034Z","iopub.status.idle":"2022-12-01T16:46:39.523738Z","shell.execute_reply":"2022-12-01T16:46:39.521602Z","shell.execute_reply.started":"2022-12-01T16:46:39.50239Z"},"trusted":true},"outputs":[],"source":["# Setting minimum fare amount to zero. \n","df['fare_amount'][df['fare_amount']<0] = 0.1\n","df[df['fare_amount']<0]"]},{"cell_type":"markdown","metadata":{},"source":["## **<span style = 'color:green'>5. Feature Engineering</span>**<a id ='Engineering'></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","There are a variety of features within the dataset and it is important to convert them into the right format such that we can analyse them easily. This would include converting datetime features and string features. As we can only feed numeric features as input to our models, our next task is to convert the features in numeric form.  Feature engineering is the process of extracting information from the existing data in order to improve the performance of the model. Feature engineering is subdivided into two parts: Feature prepocessing & Feature generation\n","\n","### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:brown;\">5.2 Feature Preprocessing</span>**\n","Feature preprocessing implies updating or transforming existing data. \n","#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Feature Transformation of pickup_datetime data type  to datetime</span>** \n","By Default all datetime based columns are considered as strings in pandas. Convert string date to datetime features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:39.526698Z","iopub.status.busy":"2022-12-01T16:46:39.52556Z","iopub.status.idle":"2022-12-01T16:46:43.568419Z","shell.execute_reply":"2022-12-01T16:46:43.567007Z","shell.execute_reply.started":"2022-12-01T16:46:39.526642Z"},"trusted":true},"outputs":[],"source":["df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime)\n","df_test['pickup_datetime'] = pd.to_datetime(df_test.pickup_datetime)"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:brown;\">5.1 Feature Generation</span>**<a id ='Feature Generation'></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","Feature generation involves creating new features from the existing data. New variables can be created as follows: "]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Datetime features</span>**\n","1. Pickup hour - from pickup_datetime\n","2. Pickup week day name\n","3. Pickup date\n","4. Pickup month\n","5. Pickup day of week in numbers. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.571167Z","iopub.status.busy":"2022-12-01T16:46:43.570477Z","iopub.status.idle":"2022-12-01T16:46:43.626264Z","shell.execute_reply":"2022-12-01T16:46:43.624786Z","shell.execute_reply.started":"2022-12-01T16:46:43.571112Z"},"trusted":true},"outputs":[],"source":["df.loc[:, 'pickup_hour'] = df['pickup_datetime'].dt.hour\n","df.loc[:, 'pickup_weekday'] = df['pickup_datetime'].dt.day_name()\n","df.loc[:, 'pickup_date'] = df['pickup_datetime'].dt.day\n","df.loc[:, 'pickup_month'] = df['pickup_datetime'].dt.month\n","df.loc[:, 'pickup_day'] = df['pickup_datetime'].dt.dayofweek\n","df_test.loc[:, 'pickup_hour'] = df_test['pickup_datetime'].dt.hour\n","df_test.loc[:, 'pickup_weekday'] = df_test['pickup_datetime'].dt.day_name()\n","df_test.loc[:, 'pickup_date'] = df_test['pickup_datetime'].dt.day\n","df_test.loc[:, 'pickup_month'] = df_test['pickup_datetime'].dt.month\n","df_test.loc[:, 'pickup_day'] = df_test['pickup_datetime'].dt.dayofweek"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Base Fare </span>**\n","[Wikipedia](https://en.wikipedia.org/wiki/Taxis_of_the_United_States#New_York_City) states that as of June 2006, fares begin at _$_ 2.50, (3.00 after 8:00 p.m., and \\$3.50 during the peak weekday hours of 4:00 - 8:00 pm). Base fare is estimated based on these slabs of time range.  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.630392Z","iopub.status.busy":"2022-12-01T16:46:43.629811Z","iopub.status.idle":"2022-12-01T16:46:43.681361Z","shell.execute_reply":"2022-12-01T16:46:43.680028Z","shell.execute_reply.started":"2022-12-01T16:46:43.630333Z"},"trusted":true},"outputs":[],"source":["def baseFare(x):\n","    if x in range(16,20):\n","        base_fare = 3.50\n","    elif x in range(20,24):\n","        base_fare = 3\n","    else:\n","        base_fare = 2.50\n","    return base_fare\n","\n","df['base_fare'] = df['pickup_hour'].apply(baseFare)\n","df_test['base_fare'] = df_test['pickup_hour'].apply(baseFare)\n","df['base_fare'], df['pickup_hour']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.683417Z","iopub.status.busy":"2022-12-01T16:46:43.682911Z","iopub.status.idle":"2022-12-01T16:46:43.693758Z","shell.execute_reply":"2022-12-01T16:46:43.691856Z","shell.execute_reply.started":"2022-12-01T16:46:43.68337Z"},"trusted":true},"outputs":[],"source":["df['fare'] = df['fare_amount'] - df['base_fare']"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Haversine Distance</span>** \n","Haversine distance: To calculate the distance (km) between pickup and dropoff points. Difference between pickup and dropoff points will give an idea about the distances covered which should be the most predictive feature for taxi fare.  The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n","\n","The haversine formula approximates the great-circle distance between two points on a sphere given their longitudes, latitudes and the sphereâs radius. The sphere we are interested in here is the Earth â which is not a perfect sphere, but close enough for the approximations that we are interested in. Important in navigation, it is a special case of a more general formula in spherical trigonometry, the law of haversines, that relates the sides and angles of spherical triangles.\n","Source: [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)\n","\n","Haversine distance can be found using geopy library, scikitleran library, or by implementing Haversine formula by defining a custom made function. All three methods are described below.  "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.697542Z","iopub.status.busy":"2022-12-01T16:46:43.695933Z","iopub.status.idle":"2022-12-01T16:46:43.81715Z","shell.execute_reply":"2022-12-01T16:46:43.815567Z","shell.execute_reply.started":"2022-12-01T16:46:43.697461Z"},"trusted":true},"outputs":[],"source":["from geopy.distance import great_circle\n","coordA=(df['pickup_latitude'][0], df['pickup_longitude'][0])\n","coordB=(df['dropoff_latitude'][0], df['dropoff_longitude'][0])\n","print (int(great_circle(coordA, coordB).kilometers))"]},{"cell_type":"markdown","metadata":{},"source":["**Calculating the Haversine distance by defining a custom made function**\n","\n","Another method is by defining a function to implement the Haversine formula in Python. Latitude and longitude need to be in radians for calculation.\n","In the function âhaversineDistanceInKMâ, first the decimal degrees are converted to radians. The return statement is a somewhat compressed version of the haversine formula implemented in python. â12734â is an approximate diameter of the earth in kilometers.\n","\n","Average Earth radius = 6371 km"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.820043Z","iopub.status.busy":"2022-12-01T16:46:43.819223Z","iopub.status.idle":"2022-12-01T16:46:43.830584Z","shell.execute_reply":"2022-12-01T16:46:43.829609Z","shell.execute_reply.started":"2022-12-01T16:46:43.820001Z"},"trusted":true},"outputs":[],"source":["#Method 1: haversineDistanceInKM\n","from math import radians, cos, sin, asin, sqrt\n","def haversineDistanceInKM(latA, lonA, latB, lonB):\n","    lonA, latA, lonB, latB = map(radians, [lonA, latA, lonB, latB])\n","    return int(12734 * asin(sqrt(\n","      sin((latB-latA)/2)**2+cos(latA)*cos(latB)*sin((lonB-lonA)/2)**2)))\n","\n","\n","latA = df['pickup_latitude'][0]\n","lonA = df['pickup_longitude'][0]\n","# Yankee stadium homeplate\n","latB = df['dropoff_latitude'][0]\n","lonB = df['dropoff_longitude'][0]\n","print(haversineDistanceInKM(latA, lonA, latB, lonB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.833241Z","iopub.status.busy":"2022-12-01T16:46:43.832753Z","iopub.status.idle":"2022-12-01T16:46:43.86149Z","shell.execute_reply":"2022-12-01T16:46:43.859693Z","shell.execute_reply.started":"2022-12-01T16:46:43.833183Z"},"trusted":true},"outputs":[],"source":["#Method 2: haversine_distance\n","def haversine_distance(lat1, lng1, lat2, lng2):\n","    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n","    AVG_EARTH_RADIUS = 6371  # in km\n","    lat = lat2 - lat1\n","    lng = lng2 - lng1\n","    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n","    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n","    return h\n","\n","df['haversine_distance'] = haversine_distance(df['pickup_latitude'].values, \n","                                                     df['pickup_longitude'].values, \n","                                                     df['dropoff_latitude'].values, \n","                                                     df['dropoff_longitude'].values)\n","df_test['haversine_distance'] = haversine_distance(df_test['pickup_latitude'].values, \n","                                                     df_test['pickup_longitude'].values, \n","                                                     df_test['dropoff_latitude'].values, \n","                                                     df_test['dropoff_longitude'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.864258Z","iopub.status.busy":"2022-12-01T16:46:43.86379Z","iopub.status.idle":"2022-12-01T16:46:43.877278Z","shell.execute_reply":"2022-12-01T16:46:43.87574Z","shell.execute_reply.started":"2022-12-01T16:46:43.864222Z"},"trusted":true},"outputs":[],"source":["df['haversine_distance'].median(), df['haversine_distance'].mean(), "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.879216Z","iopub.status.busy":"2022-12-01T16:46:43.878684Z","iopub.status.idle":"2022-12-01T16:46:43.906353Z","shell.execute_reply":"2022-12-01T16:46:43.904903Z","shell.execute_reply.started":"2022-12-01T16:46:43.879148Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Calculating the Haversine Distance using sklearn.neighbors**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:46:43.909738Z","iopub.status.busy":"2022-12-01T16:46:43.908322Z","iopub.status.idle":"2022-12-01T16:48:16.585626Z","shell.execute_reply":"2022-12-01T16:48:16.58375Z","shell.execute_reply.started":"2022-12-01T16:46:43.909692Z"},"trusted":true},"outputs":[],"source":["import sklearn.neighbors\n","dist = sklearn.neighbors.DistanceMetric.get_metric('haversine')\n","dist_miles = (dist.pairwise\n","    (np.radians(df[['pickup_latitude', 'pickup_longitude']]),\n","     np.radians(df[['dropoff_latitude','dropoff_longitude']]))*3959)\n","# Note that 3959 is the radius of the earth in miles\n","dist_km = (dist.pairwise\n","    (np.radians(df[['pickup_latitude', 'pickup_longitude']]),\n","     np.radians(df[['dropoff_latitude','dropoff_longitude']]))*6371)\n","df_dist_km = pd.DataFrame(dist_km)\n","df_dist_km.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Calculating the Haversine distance using sklearn.metrics**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:48:16.589698Z","iopub.status.busy":"2022-12-01T16:48:16.588639Z","iopub.status.idle":"2022-12-01T16:49:00.887185Z","shell.execute_reply":"2022-12-01T16:49:00.885546Z","shell.execute_reply.started":"2022-12-01T16:48:16.58965Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics.pairwise import haversine_distances\n","pickup_in_radians = np.radians(df[['pickup_latitude', 'pickup_longitude']])\n","dropoff_in_radians = np.radians(df[['dropoff_latitude','dropoff_longitude']])\n","result = pd.DataFrame(haversine_distances(pickup_in_radians, dropoff_in_radians)*6371)\n","result.head()"]},{"cell_type":"markdown","metadata":{},"source":["Extract the values in the cells on the diagonal from top-left to bottom-right of matrix using numpy.matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:00.896382Z","iopub.status.busy":"2022-12-01T16:49:00.895884Z","iopub.status.idle":"2022-12-01T16:49:04.325918Z","shell.execute_reply":"2022-12-01T16:49:04.324264Z","shell.execute_reply.started":"2022-12-01T16:49:00.896344Z"},"trusted":true},"outputs":[],"source":["mydiagonal = np.matrix.diagonal(np.array(result))\n","distance = pd.DataFrame(mydiagonal, index = df.index, columns = ['distance'])\n","distance.head()"]},{"cell_type":"markdown","metadata":{},"source":["## **<span style = 'color:green'>6. Exploratory Data Analysis (EDA)</span>**<a id ='EDA'></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:04.328338Z","iopub.status.busy":"2022-12-01T16:49:04.327831Z","iopub.status.idle":"2022-12-01T16:49:05.615588Z","shell.execute_reply":"2022-12-01T16:49:05.614076Z","shell.execute_reply.started":"2022-12-01T16:49:04.328213Z"},"trusted":true},"outputs":[],"source":["# Datetime features\n","plt.figure(figsize=(22, 6))\n","\n","# Hour of day\n","plt.subplot(221)\n","sb.countplot(df['pickup_hour'])\n","plt.xlabel('Hour of Day')\n","plt.ylabel('Total number of pickups')\n","plt.title('Hourly Variation of Total number of pickups')\n","\n","# Date\n","plt.subplot(223)\n","sb.countplot(df['pickup_date'])\n","plt.xlabel('Date')\n","plt.ylabel('Total number of pickups')\n","plt.title('Daily Variation of Total number of pickups')\n","\n","# Day of week\n","plt.subplot(222)\n","sb.countplot(df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'])\n","plt.xlabel('Week Day')\n","plt.ylabel('Total Number of pickups')\n","plt.title('Weekly Variation of Total number of pickups')\n","\n","# Month\n","plt.subplot(224)\n","sb.countplot(df['pickup_month'])\n","plt.xlabel('Month')\n","plt.ylabel('Total number of pickups')\n","plt.title('Monthly Variation of Total number of pickups');"]},{"cell_type":"markdown","metadata":{},"source":["**Observations:**\n","* Eventhogh, contrary to the expectation, number of pickups is much lower during the morning peak hours, it is highest in late evenings as expected. \n","* Number of pickups on sundays and mondays are much lower than other days with a peak on Saturday. \n","* Monthly plot shows significant variation towards the end of the month. \n","* Annual variation visible with first half of the year showing higher trips compared to the lower half. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:05.61914Z","iopub.status.busy":"2022-12-01T16:49:05.617602Z","iopub.status.idle":"2022-12-01T16:49:06.039994Z","shell.execute_reply":"2022-12-01T16:49:06.038517Z","shell.execute_reply.started":"2022-12-01T16:49:05.619082Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(22, 6))\n","#fig, axs = plt.subplot(ncols=2)\n","\n","# Passenger Count\n","plt.subplot(121)\n","sb.countplot(df['passenger_count'])\n","plt.xlabel('Passenger Count')\n","plt.ylabel('Frequency')\n","plt.title('Frequency Distribution of Passenger Count')\n","\n","plt.subplot(122)\n","sb.boxplot(df['passenger_count'], color = 'cyan', showmeans=True, \n","           meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \n","                      \"markeredgecolor\":\"black\",\"markersize\":\"10\"}\n",")\n","plt.xlabel('Passenger Count')\n","plt.title('Box plot of Passenger count');"]},{"cell_type":"markdown","metadata":{},"source":["**Findings:** \n","\n","Most of the trips involve only 1 passenger. There are trips with zero passengers but they are very low in number."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:06.042637Z","iopub.status.busy":"2022-12-01T16:49:06.041872Z","iopub.status.idle":"2022-12-01T16:49:06.441823Z","shell.execute_reply":"2022-12-01T16:49:06.439767Z","shell.execute_reply.started":"2022-12-01T16:49:06.042589Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(22, 6))\n","sb.boxplot(x = df['passenger_count'],y = df['fare_amount'], color = 'cyan', showmeans=True, \n","            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"}\n",")\n","plt.xlabel('Passenger Count')\n","plt.title (\"Fare amount vs No. of passengers\");"]},{"cell_type":"markdown","metadata":{},"source":["Box plot shows no significant vatiation means  of between passenger counts."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:06.445104Z","iopub.status.busy":"2022-12-01T16:49:06.443813Z","iopub.status.idle":"2022-12-01T16:49:06.835343Z","shell.execute_reply":"2022-12-01T16:49:06.833763Z","shell.execute_reply.started":"2022-12-01T16:49:06.445033Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 6))\n","sb.boxplot(x = df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'],y = df['passenger_count'], color = 'cyan', showmeans=True, \n","            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"}\n",")\n","plt.xlabel('Passenger Count')\n","plt.title (\"No. of passengers vs Days of week\");"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:06.838228Z","iopub.status.busy":"2022-12-01T16:49:06.837688Z","iopub.status.idle":"2022-12-01T16:49:11.737438Z","shell.execute_reply":"2022-12-01T16:49:11.735596Z","shell.execute_reply.started":"2022-12-01T16:49:06.838188Z"},"trusted":true},"outputs":[],"source":["# Datetime features\n","plt.figure(figsize=(22, 8))\n","\n","# Hour of day\n","plt.subplot(221)\n","sb.barplot(df['pickup_hour'], y = df['passenger_count'], palette = 'hsv')\n","plt.xlabel('Hour of Day')\n","plt.ylabel('Passenger count')\n","plt.title (\"Passenger count vs Hour of Day\")\n","\n","# Day of week\n","plt.subplot(222)\n","sb.barplot(df['pickup_month'], y = df['passenger_count'],palette = 'hsv')\n","plt.xlabel('Month')\n","plt.ylabel('Passenger count')\n","plt.title (\"Passenger count vs Month\")\n","\n","# Date\n","plt.subplot(223)\n","sb.barplot(x = df['pickup_date'], y = df['passenger_count'], palette = 'hsv')\n","plt.xlabel('Date')\n","plt.ylabel('Passenger count')\n","plt.title (\"Passenger count vs Date\")\n","\n","# Month\n","plt.subplot(224)\n","sb.barplot(x = df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'],\n","           y = df['passenger_count'], palette = 'hsv')\n","plt.xlabel('Days of week')\n","plt.ylabel('Passenger count')\n","plt.title (\"Passenger count vs Days of week\")\n","plt.tight_layout();"]},{"cell_type":"markdown","metadata":{},"source":["* Passenger count is more at night from 22:00 to 03:00\n","* Passenger count is more on weekdays, compared to weekends"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:11.739972Z","iopub.status.busy":"2022-12-01T16:49:11.739523Z","iopub.status.idle":"2022-12-01T16:49:12.120372Z","shell.execute_reply":"2022-12-01T16:49:12.118394Z","shell.execute_reply.started":"2022-12-01T16:49:11.739933Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15, 6))\n","sb.boxplot(x = df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'],\n","           y = df['fare_amount'], palette = 'rainbow', showmeans=True, \n","            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"Red\", \"markeredgecolor\":\"black\",\n","                       \"markersize\":\"10\"}\n",")\n","plt.xlabel('Fare amount')\n","plt.title (\"Fare amount vs Days of week\");"]},{"cell_type":"markdown","metadata":{},"source":["No significant variation of means between days of week also. \n","\n","#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Dealing with Distance Outliers</span>** \n","\n","Outliers can be dealt in three different ways as follows:\n","1. Replace using central tendancy (median for continuous values and mode for categorical values)\n","2. Replace using Whisker values\n","3. Selecting only those within whisker values. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.124411Z","iopub.status.busy":"2022-12-01T16:49:12.123197Z","iopub.status.idle":"2022-12-01T16:49:12.564886Z","shell.execute_reply":"2022-12-01T16:49:12.563434Z","shell.execute_reply.started":"2022-12-01T16:49:12.124354Z"},"trusted":true},"outputs":[],"source":["sb.distplot(df['haversine_distance'], bins = 20);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.567115Z","iopub.status.busy":"2022-12-01T16:49:12.566678Z","iopub.status.idle":"2022-12-01T16:49:12.585169Z","shell.execute_reply":"2022-12-01T16:49:12.583593Z","shell.execute_reply.started":"2022-12-01T16:49:12.567076Z"},"trusted":true},"outputs":[],"source":["df['haversine_distance'].describe(), print(\"Median       \", df['haversine_distance'].median())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.587145Z","iopub.status.busy":"2022-12-01T16:49:12.586754Z","iopub.status.idle":"2022-12-01T16:49:12.601996Z","shell.execute_reply":"2022-12-01T16:49:12.599996Z","shell.execute_reply.started":"2022-12-01T16:49:12.58711Z"},"trusted":true},"outputs":[],"source":["df['haversine_distance'].quantile(0.25), df['haversine_distance'].quantile(0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.606548Z","iopub.status.busy":"2022-12-01T16:49:12.605575Z","iopub.status.idle":"2022-12-01T16:49:12.622319Z","shell.execute_reply":"2022-12-01T16:49:12.620492Z","shell.execute_reply.started":"2022-12-01T16:49:12.60647Z"},"trusted":true},"outputs":[],"source":["IQR = df['haversine_distance'].quantile(0.75) - df['haversine_distance'].quantile(0.25)\n","IQR"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.624605Z","iopub.status.busy":"2022-12-01T16:49:12.624005Z","iopub.status.idle":"2022-12-01T16:49:12.641726Z","shell.execute_reply":"2022-12-01T16:49:12.63979Z","shell.execute_reply.started":"2022-12-01T16:49:12.624565Z"},"trusted":true},"outputs":[],"source":["Q1 = df['haversine_distance'].quantile(0.25)\n","Q3 = df['haversine_distance'].quantile(0.75)\n","whisker_1 = Q1 - (1.5*IQR)\n","whisker_2 = Q3 + (1.5*IQR)\n","\n","whisker_1, whisker_2"]},{"cell_type":"markdown","metadata":{},"source":["Replacing outliers with whisker values is one of the methods of treating outliers. Values at the lower end should be imputed by the lower whisker (i.e., Q1-1.5* IQR) and the values at the upper end should be imputed by the upper whisker (ie., Q3+1.5* IQR). Here selecting only those rows containing  distance values within upper whisker. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.643973Z","iopub.status.busy":"2022-12-01T16:49:12.643431Z","iopub.status.idle":"2022-12-01T16:49:12.659231Z","shell.execute_reply":"2022-12-01T16:49:12.657601Z","shell.execute_reply.started":"2022-12-01T16:49:12.643935Z"},"trusted":true},"outputs":[],"source":["df = df.loc[(df['haversine_distance']!=0) & (df['haversine_distance']<8)]\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:12.662816Z","iopub.status.busy":"2022-12-01T16:49:12.661664Z","iopub.status.idle":"2022-12-01T16:49:13.0663Z","shell.execute_reply":"2022-12-01T16:49:13.064821Z","shell.execute_reply.started":"2022-12-01T16:49:12.662767Z"},"trusted":true},"outputs":[],"source":["sb.distplot(df['haversine_distance'], bins = 20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Target Exploration with distance**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:13.068568Z","iopub.status.busy":"2022-12-01T16:49:13.068109Z","iopub.status.idle":"2022-12-01T16:49:15.215952Z","shell.execute_reply":"2022-12-01T16:49:15.21455Z","shell.execute_reply.started":"2022-12-01T16:49:13.06853Z"},"trusted":true},"outputs":[],"source":["from scipy import stats\n","x = df['haversine_distance']\n","y = df['fare_amount']\n","slope, intercept, r_value, p_value, std_err = stats.linregress(df['haversine_distance'],df['fare_amount'])\n","ax = sb.regplot(x, y, line_kws={'label':\"y={0:.1f}x+{1:.1f}\".format(slope,intercept), \n","                                \"color\": \"red\"},scatter_kws={\"color\": \"cyan\"})\n","ax.legend();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:15.217956Z","iopub.status.busy":"2022-12-01T16:49:15.217548Z","iopub.status.idle":"2022-12-01T16:49:15.710216Z","shell.execute_reply":"2022-12-01T16:49:15.708402Z","shell.execute_reply.started":"2022-12-01T16:49:15.21792Z"},"trusted":true},"outputs":[],"source":["#sb.lmplot(x=\"haversine_distance\", y=\"fare_amount\", data=df );\n","sb.relplot(x=\"haversine_distance\", y=\"fare_amount\", data=df, kind=\"scatter\");"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:15.712928Z","iopub.status.busy":"2022-12-01T16:49:15.712375Z","iopub.status.idle":"2022-12-01T16:49:20.155446Z","shell.execute_reply":"2022-12-01T16:49:20.152617Z","shell.execute_reply.started":"2022-12-01T16:49:15.71288Z"},"trusted":true},"outputs":[],"source":["# Datetime features\n","plt.figure(figsize=(22, 8))\n","\n","# Hour of day\n","plt.subplot(221)\n","sb.barplot(df['pickup_hour'], y = df['haversine_distance'], palette = 'tab20')\n","plt.xlabel('Hour of Day')\n","plt.ylabel('Distance in Km')\n","plt.title (\"Distance in Km vs Hour of Day\")\n","\n","# Day of week\n","plt.subplot(222)\n","sb.barplot(df['pickup_month'], y = df['haversine_distance'],palette = 'tab20',estimator = np.mean)\n","plt.xlabel('Month')\n","plt.ylabel('Distance in Km')\n","plt.title (\"Distance in Km vs Month\")\n","\n","# Date\n","plt.subplot(223)\n","sb.barplot(x = df['pickup_date'], y = df['haversine_distance'], palette = 'tab20')\n","plt.xlabel('Date')\n","plt.ylabel('Distance in Km')\n","plt.title (\"Distance in Km vs Date\")\n","\n","# Month\n","plt.subplot(224)\n","sb.barplot(x = df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'],\n","           y = df['haversine_distance'], palette = 'tab10')\n","plt.xlabel('Days of week')\n","plt.ylabel('Distance in Km')\n","plt.title (\"Distance in Km vs Days of week\")\n","plt.tight_layout();"]},{"cell_type":"markdown","metadata":{},"source":["Distance travelled is comparitively lower during daytime than at night."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:20.158785Z","iopub.status.busy":"2022-12-01T16:49:20.158204Z","iopub.status.idle":"2022-12-01T16:49:24.314721Z","shell.execute_reply":"2022-12-01T16:49:24.312848Z","shell.execute_reply.started":"2022-12-01T16:49:20.158735Z"},"trusted":true},"outputs":[],"source":["# Datetime features\n","plt.figure(figsize=(22, 8))\n","\n","# Hour of day\n","plt.subplot(221)\n","sb.barplot(df['pickup_hour'], y = df['fare_amount'], palette = 'tab20')\n","plt.xlabel('Hour of Day')\n","plt.ylabel('Fare amount')\n","plt.title (\"Fare amount vs Hour of Day\")\n","\n","# Day of week\n","plt.subplot(222)\n","sb.barplot(df['pickup_month'], y = df['fare_amount'],palette = 'tab20')\n","plt.xlabel('Month')\n","plt.ylabel('Fare amount')\n","plt.title (\"Fare amount vs Month\")\n","\n","# Date\n","plt.subplot(223)\n","sb.barplot(x = df['pickup_date'], y = df['fare_amount'], palette = 'tab20')\n","plt.xlabel('Date')\n","plt.ylabel('Fare amount')\n","plt.title (\"Fare amount vs Date\")\n","\n","# Month\n","plt.subplot(224)\n","sb.barplot(x = df['pickup_weekday'], order = ['Monday', 'Tuesday', 'Wednesday', \n","                                           'Thursday', 'Friday', 'Saturday', 'Sunday'],\n","           y = df['fare_amount'], palette = 'tab10')\n","plt.xlabel('Days of week')\n","plt.ylabel('Fare amount')\n","plt.title (\"Fare amount vs Days of week\")\n","plt.tight_layout();"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Lattitude & Longitude</span>** \n","Lets look at the geospatial or location features to check consistency. They should not vary much as we are only considering trips within New York city."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:24.317179Z","iopub.status.busy":"2022-12-01T16:49:24.316769Z","iopub.status.idle":"2022-12-01T16:49:26.322717Z","shell.execute_reply":"2022-12-01T16:49:26.320926Z","shell.execute_reply.started":"2022-12-01T16:49:24.317144Z"},"trusted":true},"outputs":[],"source":["f, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)\n","sb.despine(left=True)\n","sb.distplot(df['pickup_latitude'].values, label = 'pickup_latitude',color=\"b\",bins = 100, ax=axes[0,0])\n","sb.distplot(df['pickup_longitude'].values, label = 'pickup_longitude',color=\"r\",bins =100, ax=axes[1,0])\n","sb.distplot(df['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"b\",bins =100, ax=axes[0,1])\n","sb.distplot(df['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"r\",bins =100, ax=axes[1,1])\n","plt.setp(axes, yticks=[])\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Findings** - (Here, red represents pickup and dropoff Longitudes & blue represents pickup & dropoff lattitudes)\n","\n","1. From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 to -73. \n","2. Some extreme co-ordinates has squeezed the plot such that we see a spike here\n","3. A good idea is to remove these outliers and look at the distribution more closely"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:26.325887Z","iopub.status.busy":"2022-12-01T16:49:26.325318Z","iopub.status.idle":"2022-12-01T16:49:28.959666Z","shell.execute_reply":"2022-12-01T16:49:28.957978Z","shell.execute_reply.started":"2022-12-01T16:49:26.325837Z"},"trusted":true},"outputs":[],"source":["df = df.loc[(df.pickup_latitude > 40.6) & (df.pickup_latitude < 40.9)]\n","df = df.loc[(df.dropoff_latitude>40.6) & (df.dropoff_latitude < 40.9)]\n","df = df.loc[(df.dropoff_longitude > -74.05) & (df.dropoff_longitude < -73.7)]\n","df = df.loc[(df.pickup_longitude > -74.05) & (df.pickup_longitude < -73.7)]\n","df_data_new = df.copy()\n","sb.set(style=\"white\", palette=\"muted\", color_codes=True)\n","f, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)#\n","sb.despine(left=True)\n","sb.distplot(df_data_new['pickup_latitude'].values, label = 'pickup_latitude',color=\"b\",bins = 100, ax=axes[0,0])\n","sb.distplot(df_data_new['pickup_longitude'].values, label = 'pickup_longitude',color=\"r\",bins =100, ax=axes[0,1])\n","sb.distplot(df_data_new['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"b\",bins =100, ax=axes[1, 0])\n","sb.distplot(df_data_new['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"r\",bins =100, ax=axes[1, 1])\n","plt.setp(axes, yticks=[])\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["* We have a much better view of the distribution of coordinates instead of spikes. And we see that most trips are concentrated between these lat long only with a few significant clusters.\n","* These clusters are represented by the numerous peaks in the lattitude and longitude histograms.\n","\n","### **<span style = 'color:brown'>Distribution Plot of Target Variable</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:28.962171Z","iopub.status.busy":"2022-12-01T16:49:28.961697Z","iopub.status.idle":"2022-12-01T16:49:32.742646Z","shell.execute_reply":"2022-12-01T16:49:32.741492Z","shell.execute_reply.started":"2022-12-01T16:49:28.96213Z"},"trusted":true},"outputs":[],"source":["import holoviews as hv\n","from holoviews import opts\n","hv.extension('bokeh')\n","hv.Distribution(df['fare_amount']).opts(title=\"Fare Amount Distribution\", color=\"red\",\n","                                                        xlabel=\"Fare Amount\", ylabel=\"Density\")\\\n",".opts(opts.Distribution(width=700, height=300,tools=['hover'],show_grid=True))"]},{"cell_type":"markdown","metadata":{},"source":["[Multicollinearity](https://www.statology.org/how-to-calculate-vif-in-python/) in regression analysis occurs when two or more explanatory variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model. \n","\n","One way to detect multicollinearity is by using a metric known as the variance inflation factor (VIF), which measures the correlation and strength of correlation between the explanatory variables in a regression model. The value for VIF starts at 1 with no upper limit. \n","\n","* VIF value of 1 indicates no multicollinearity\n","* VIF values between 1to 5 indicates moderate multicollinearity, though not severe enough to pay attention\n","* VIF value greater than 5 indicates potentially severe multicollinearity\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:32.746227Z","iopub.status.busy":"2022-12-01T16:49:32.744337Z","iopub.status.idle":"2022-12-01T16:49:33.060443Z","shell.execute_reply":"2022-12-01T16:49:33.059Z","shell.execute_reply.started":"2022-12-01T16:49:32.746164Z"},"trusted":true},"outputs":[],"source":["from patsy import dmatrices\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","# the independent variables set\n","X =df.drop(['key', 'pickup_datetime','pickup_weekday', 'fare_amount', 'base_fare', 'fare'], axis = 1)\n","\n","# VIF dataframe\n","vif_data = pd.DataFrame()\n","vif_data[\"feature\"] = X.columns\n","  \n","# calculating VIF for each feature\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","  \n","print(vif_data)"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, pickup_latitude, pickup_longitude, dropoff_longitude and dropoff_latitude have very high values of VIF. A value greater than 5 indicates potentially severe correlation between a given explanatory variable and other explanatory variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable. Hence, it will be better to drop two of these four variables from the model. "]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Correlation matrix</span>**\n","Correlation heatmap to check the correlations amongst all features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:33.06981Z","iopub.status.busy":"2022-12-01T16:49:33.062363Z","iopub.status.idle":"2022-12-01T16:49:34.486712Z","shell.execute_reply":"2022-12-01T16:49:34.485583Z","shell.execute_reply.started":"2022-12-01T16:49:33.069707Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize = (12,6))\n","sb.heatmap(df.drop(['key', 'pickup_datetime','pickup_weekday'], axis = 1).corr(), \n","           cmap ='BuGn', annot = True);"]},{"cell_type":"markdown","metadata":{},"source":["From the correlation heatmap it is clear that the distance, lattitude and longitude features have higher correlation with the target as compared to the other features."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.489216Z","iopub.status.busy":"2022-12-01T16:49:34.488549Z","iopub.status.idle":"2022-12-01T16:49:34.508756Z","shell.execute_reply":"2022-12-01T16:49:34.507617Z","shell.execute_reply.started":"2022-12-01T16:49:34.489174Z"},"trusted":true},"outputs":[],"source":["df[['pickup_latitude', 'pickup_longitude', 'dropoff_longitude', 'dropoff_latitude']].corr()"]},{"cell_type":"markdown","metadata":{},"source":["AS seen from the above correlation table, dropoff latitude is having high correlation with pickup latitude; the same is true for longitude also. So dropoff latitude and longitude is going to be dropped from the analysis, as only one each among latitude and longitude is required. "]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Data scaling & Train Test split</span>** "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.510981Z","iopub.status.busy":"2022-12-01T16:49:34.510143Z","iopub.status.idle":"2022-12-01T16:49:34.537285Z","shell.execute_reply":"2022-12-01T16:49:34.535189Z","shell.execute_reply.started":"2022-12-01T16:49:34.510933Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["We have all numerical data types in our dataset now. Time to delve into Standardization followed by model building."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.539705Z","iopub.status.busy":"2022-12-01T16:49:34.539274Z","iopub.status.idle":"2022-12-01T16:49:34.547469Z","shell.execute_reply":"2022-12-01T16:49:34.546454Z","shell.execute_reply.started":"2022-12-01T16:49:34.539667Z"},"trusted":true},"outputs":[],"source":["X = df.drop(['key', 'pickup_datetime','pickup_weekday', 'fare_amount', 'fare', 'base_fare', 'dropoff_latitude', 'dropoff_longitude'], \n","            axis = 1)\n","y = df['fare_amount']"]},{"cell_type":"markdown","metadata":{},"source":["Data Standardization give data zero mean and unit variance "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.549675Z","iopub.status.busy":"2022-12-01T16:49:34.549014Z","iopub.status.idle":"2022-12-01T16:49:34.574029Z","shell.execute_reply":"2022-12-01T16:49:34.572774Z","shell.execute_reply.started":"2022-12-01T16:49:34.549636Z"},"trusted":true},"outputs":[],"source":["from sklearn import preprocessing\n","X= preprocessing.StandardScaler().fit(X).transform(X)\n","X[0:5]"]},{"cell_type":"markdown","metadata":{},"source":["Keep one third of the data in test set and remaining two third of data in the train set. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.576817Z","iopub.status.busy":"2022-12-01T16:49:34.576263Z","iopub.status.idle":"2022-12-01T16:49:34.591216Z","shell.execute_reply":"2022-12-01T16:49:34.589707Z","shell.execute_reply.started":"2022-12-01T16:49:34.576767Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","print(X_train.ndim)\n","print(y_train.ndim)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Mean Prediction</span>** \n","Before we go on to try any machine learning model, let us look at the performance of a basic model that just says the mean of fare amount in the train set is the prediction for all the trips in the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.59355Z","iopub.status.busy":"2022-12-01T16:49:34.593041Z","iopub.status.idle":"2022-12-01T16:49:34.606848Z","shell.execute_reply":"2022-12-01T16:49:34.605464Z","shell.execute_reply.started":"2022-12-01T16:49:34.593501Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","mean_pred = np.repeat(y_train.mean(),len(y_test))\n","sqrt(mean_squared_error(y_test, mean_pred))"]},{"cell_type":"markdown","metadata":{},"source":["## **<span style = 'color:green'>7. Model Development**<a name ='Model'></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table) \n"]},{"cell_type":"markdown","metadata":{},"source":["The following algorithms are going to be used to build regression models:<br>\n","* Linear Regressor\n","* SGDRegressor\n","* Ridge Regressor\n","* Lasso Regressor\n","* ElasticNet Regressor\n","* KNeighbors Regressor\n","* Support Vector Regressor\n","* Decision Tree Regressor\n","* Extra Trees Regressor\n","* Isolation Forest Regressor\n","* Random Forest Regressor\n","* Bagging Regressor\n","* AdaBoost Regressor\n","* Gradient Boosting Regressor\n","* XGB Regressor\n","* CatBoost Regressor\n","* MLPRegressor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:34.609886Z","iopub.status.busy":"2022-12-01T16:49:34.60831Z","iopub.status.idle":"2022-12-01T16:49:47.20236Z","shell.execute_reply":"2022-12-01T16:49:47.200403Z","shell.execute_reply.started":"2022-12-01T16:49:34.609825Z"},"trusted":true},"outputs":[],"source":["pip install -q --upgrade linear-tree"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Cross Validation(CV)</span>** \n","Cross Validation(CV) or K-Fold Cross Validation (K-Fold CV) is used to generate multiple (k) train-test sets instead of 1. In a k fold cross-validation, the training data will be divided into k equal parts. In the first step, one part out of the k is set as validation data and the remaining k-1 as train data.  This is repeated k times using a different part out of the k, each time to test the model upon.  K-fold cross validation can essentially help to combat overfitting too. \n","Different regression models can be evaluated based on the CV scores. The cross_val_score for regression calculates the R squared metric for the applied model. R squared error close to 1 implies a better fit and less error.\n","\n","#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Linear Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","A linear relationship is one in which increment / decrement in one variable leads to the increment/decrement of the other. A Linear regression model expresses relationship between dependant and independant variables which can be demonstrated by a straight line.   \n","\n","* Y = mX + C is the equation of a straight line, where, Y is the dependent variable, X is the independent variable,\n","* C refers to the intercept of the regression line0, in other words: the value of Y when X is 0\n","* m refers to the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit.\n","\n","* Cost function of Linear Regression is Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) between the predicted value and the actual value. Lower the cost function better the model. \n","* Gradient descent is the optimization technique  used by various supervised machine learning algorithms including  Linear regression to minimize the cost function in order to get the best values of m and C. Gradient descent works iteratively calculating error at each term, moving in the direction of lower error by optimizing model parameters until model converges to minimal cost. \n","* We want gradient descent to reach the global minima of a convex cost function in order to get the optimal model. Random initialization and adjusting learning rate can help us in reaching the global minima of the function to find the best model. \n","\n","Assumptions of Linear Regression:\n","* Linear Relationship between dependent and independent variables.\n","* No correlation of error terms\n","* Constant variance of error terms\n","* No correlation among independent variables,\n","* Errors normally distributed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:47.204976Z","iopub.status.busy":"2022-12-01T16:49:47.204477Z","iopub.status.idle":"2022-12-01T16:49:47.363472Z","shell.execute_reply":"2022-12-01T16:49:47.361163Z","shell.execute_reply.started":"2022-12-01T16:49:47.204936Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()\n","np.mean(cross_val_score(lr, X_train, y_train, cv=5))"]},{"cell_type":"markdown","metadata":{},"source":["Use model_selection.cross_validate (with return_estimator=True) instead of cross_val_score. It's a lot more flexible so the estimators used for each fold can be accessed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:47.381537Z","iopub.status.busy":"2022-12-01T16:49:47.376766Z","iopub.status.idle":"2022-12-01T16:49:47.493989Z","shell.execute_reply":"2022-12-01T16:49:47.492148Z","shell.execute_reply.started":"2022-12-01T16:49:47.381411Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(lr, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","df_train = df.drop(['key', 'pickup_datetime','pickup_weekday', 'fare_amount', 'fare', 'base_fare', \n","                    'dropoff_latitude', 'dropoff_longitude'], axis = 1)\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Ridge Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","A regression model that uses L2 regularization technique is called  Ridge Regression. When more features are added to a linear regression model, it can lead to overfitting, where the model performs well on train but not on test dataset. Values of coefficients also get larger. In ridge regression, a regularization term, which is a sum of \"squared magnitude\" of all coefficients, is added as penalty to the cost function of linear regression algorithm. This decreases the coefficients values significantly and hence the effect of least significant features gets reduced. \n","\n","Here, if regularization term, lambda is zero then the model get back to OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that itâs important how lambda is chosen. This technique works very well to avoid over-fitting issue."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:47.508323Z","iopub.status.busy":"2022-12-01T16:49:47.503314Z","iopub.status.idle":"2022-12-01T16:49:47.726696Z","shell.execute_reply":"2022-12-01T16:49:47.724431Z","shell.execute_reply.started":"2022-12-01T16:49:47.508205Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import RidgeCV\n","ridge = RidgeCV(cv=5).fit(X_train, y_train)\n","ridge.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:47.742772Z","iopub.status.busy":"2022-12-01T16:49:47.736611Z","iopub.status.idle":"2022-12-01T16:49:48.512007Z","shell.execute_reply":"2022-12-01T16:49:48.510192Z","shell.execute_reply.started":"2022-12-01T16:49:47.742617Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(ridge, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Lasso Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","Lasso (Least Absolute Shrinkage and Selection Operator) regression is a type of linear regression similar to Ridge regression. In Lasso, L1 regularization, which is a sum of âabsolute values of magnitudeâ of all coeffients are applied as a penalty to the cost function. In Ridge regression, coefficients go down, but none of them all the way up to zero. So there will be difficulty in interpretation because we still have the same number of parameters even though with less intensity. Compared to that, Lasso can give us less number of parameters or less number of features. Here some of coefficients are actually dropped from the model. Thus the key difference between these techniques is that Lasso shrinks the less important featureâs coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n","\n","Again, if lambda is zero then the model get back to OLS whereas very large value will make coefficients zero hence it will under-fit."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:48.526562Z","iopub.status.busy":"2022-12-01T16:49:48.521214Z","iopub.status.idle":"2022-12-01T16:49:48.84767Z","shell.execute_reply":"2022-12-01T16:49:48.845809Z","shell.execute_reply.started":"2022-12-01T16:49:48.526439Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LassoCV\n","lasso = LassoCV(cv=5).fit(X_train, y_train)\n","lasso.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:48.861241Z","iopub.status.busy":"2022-12-01T16:49:48.856057Z","iopub.status.idle":"2022-12-01T16:49:50.354821Z","shell.execute_reply":"2022-12-01T16:49:50.352845Z","shell.execute_reply.started":"2022-12-01T16:49:48.86114Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(lasso, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Elasticnet Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","This is also a type of linear regression in which both Lasso (L1) and Ridge (L2) regularization parameters combined together are added to the cost function. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:50.358567Z","iopub.status.busy":"2022-12-01T16:49:50.357029Z","iopub.status.idle":"2022-12-01T16:49:50.716323Z","shell.execute_reply":"2022-12-01T16:49:50.714543Z","shell.execute_reply.started":"2022-12-01T16:49:50.358503Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import ElasticNetCV\n","elastic = ElasticNetCV(cv=5).fit(X_train, y_train)\n","elastic.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:50.722003Z","iopub.status.busy":"2022-12-01T16:49:50.719806Z","iopub.status.idle":"2022-12-01T16:49:52.167713Z","shell.execute_reply":"2022-12-01T16:49:52.166246Z","shell.execute_reply.started":"2022-12-01T16:49:50.721937Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(elastic, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Polynomial Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:52.178192Z","iopub.status.busy":"2022-12-01T16:49:52.174586Z","iopub.status.idle":"2022-12-01T16:49:54.161228Z","shell.execute_reply":"2022-12-01T16:49:54.159596Z","shell.execute_reply.started":"2022-12-01T16:49:52.178105Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","cv_score=[]\n","for i in range(1,4):\n","    poly_reg = PolynomialFeatures(degree = i)\n","    X_poly = poly_reg.fit_transform(X_train)\n","    poly_reg = LinearRegression()\n","    cv_score.append(np.mean(cross_val_score(poly_reg,X_poly,y_train,cv=5)))\n","x = range(1,4)\n","plt.scatter(x,cv_score)\n","plt.xticks(ticks=[1,2,3], labels=['Degree_1', 'Degree_2', 'Degree_3']);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:54.163762Z","iopub.status.busy":"2022-12-01T16:49:54.163317Z","iopub.status.idle":"2022-12-01T16:49:54.313216Z","shell.execute_reply":"2022-12-01T16:49:54.311709Z","shell.execute_reply.started":"2022-12-01T16:49:54.163727Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(poly_reg, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">K-Nearest Neighbor (KNN)</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","K- Nearest Neighbor (KNN), also known as Lazy learning algorithm observes the nature of nearest neighbors. KNN is a supervised ML algorithm.  In the KNN algorithm, âKâ means the number of nearest neighbors the model will consider. If it is a classification problem then we assign a mode to the instance and if it is a regression problem then we assign a mean or median of the distances of K nearest neighbors to the new unknown variable. . \n","\n","Determining value of K:\n","* Choose a range of values for k\n","* For each value, implement a KNN model\n","* Calculate error (Or, R_squared value) corresponding to each K value and plot it.  \n","\n","The elbow curve is used to find the value of k to be used in the model. The value of k where the error is minimum gets selected. R_squared value is inversely proportional to the error. Higher the R2 score better will be the model. \n","\n","How to calculate the distance?\n","* Manhattan Distance: Sum of Absolute difference between the two points, across all dimensions.\n","* Euclidean Distance is the shortest distances between two points. It is the square root of  sum of squares of distance between two points.\n","* Minkowski distance is a generalized version of the above distance calculations. It is the p-th root of sum of squares of distance between two points.\n","* Hamming distance is the total number of differences between two strings of identical length. \n","\n","Hamming distance is used for categorical data whereas Manhattan distance and Euclidian distance are for continuous data. \n","\n","Manhattan distances can be thought of as the sum of the sides of a right-angled triangle while Euclidean distances represent the hypotenuse of the triangle. Hence, Manhattan distances are usually larger than Euclidean distances. In the Minkowski equation, when p=2, we get our familar Euclidian distance (also referred to as the L2-norm or L2 Distance). When p equals 1 we get the Manhattan distance. Manhattan distances can also find them selves called L1 norm, L1 distance, and even LASSO. \n","\n","As p tends to infinity we get another famous distance, the Chebyshev distance. Manhattan, Euclidean, Chebyshev, and Minkowski distances are part of the scikit-learn DistanceMetric class and can be used to tune regressors / classifiers such as KNN or clustering alogorithms such as DBSCAN."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:49:54.318418Z","iopub.status.busy":"2022-12-01T16:49:54.315644Z","iopub.status.idle":"2022-12-01T16:50:03.918222Z","shell.execute_reply":"2022-12-01T16:50:03.916566Z","shell.execute_reply.started":"2022-12-01T16:49:54.318324Z"},"trusted":true},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","cv_score=[]\n","for i in range(1,10):\n"," knn = KNeighborsRegressor(n_neighbors= i)\n"," cv_score.append(np.mean(cross_val_score(knn,X_train, y_train,cv=5)))\n","x = range(1,10)\n","plt.scatter(x,cv_score);"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Decision Tree</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","\n","A decision tree is a type of machine learning model that is used to predict the value of a target variable. Decision trees are created by splitting the data set into smaller and smaller subsets, in such a way that the resultamt nodes are as homogeneous as possible, until each subset contains only one data point. So the objective of the decision tree is to have pure nodes which contains 100% of one class and 0% of the other classes. When a sub node splits into further sub nodes, then it is known as decision nodes.  The terminal node (or leaves) lies at the bottom of the decision tree. It has no further nodes coming off.  The distance between parent root node and the longest terminal leaf nodes is called the depth of the tree. \n","\n","Decision tree splits the nodes on all available variables. It selects the split which results in most homogeneous sub-nodes. The best split is decided based on Gini impurity, Chi-Square,  Entropy / Information Gain or reduction in variance. The first three parameters are used when target variable is categorical and reduction in variance is uded for continuous variable.\n","\n","Decision tree regression using reduction in variance selects the split with lower variance. It calculate the variance at each split as weighted average variance of each child node. A node having high variance means it is more impure. Since we seek the pure nodes after splitting the variable having low variance should be selected for splitting. \n","\n","Optimizing parameters:\n","\n","1. Minimum samples for a node split\n","    > a. Higher values controls overfitting\n","    \n","    > b. Too high values can lead to underfitting\n","2. Minimum samples for a terminal node\n","    > a. Higher values controls overfitting\n","    \n","    > b. Too high values can lead to underfitting\n","3. Maximum depth of tree\n","    > a. Higher depth can lead to overfitting\n","    \n","    > b.Lower depth can lead to underfitting\n","4. Maximum number of terminal nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:50:03.920231Z","iopub.status.busy":"2022-12-01T16:50:03.919852Z","iopub.status.idle":"2022-12-01T16:50:05.057734Z","shell.execute_reply":"2022-12-01T16:50:05.056009Z","shell.execute_reply.started":"2022-12-01T16:50:03.920198Z"},"trusted":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","DT = DecisionTreeRegressor()\n","R_Squared = np.mean(cross_val_score(DT, X_train, y_train, cv=5))\n","Standard_deviation = np.std(cross_val_score(DT, X_train, y_train, cv=5))\n","print('R2 of Decision Tree Regression model is:',R_Squared)\n","print('Standard deviation of R2 of Decision Tree Regression model is:',Standard_deviation)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:50:05.062219Z","iopub.status.busy":"2022-12-01T16:50:05.060795Z","iopub.status.idle":"2022-12-01T16:51:13.303173Z","shell.execute_reply":"2022-12-01T16:51:13.301234Z","shell.execute_reply.started":"2022-12-01T16:50:05.062172Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","rf = RandomForestRegressor()\n","R_Squared = np.mean(cross_val_score(rf, X_train, y_train, cv=5))\n","Standard_deviation = np.std(cross_val_score(rf, X_train, y_train, cv=5))\n","print('R2 of Random Forest Regression model is:',R_Squared)\n","print('Standard deviation of R2 of Random Forest Regression model is:',Standard_deviation)"]},{"cell_type":"markdown","metadata":{},"source":["#### **<span style=\"font-family: Segoe UI; font-size:1.0em;color:magenta;\">Gradient boosting Regression</span>** \n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table)\n","Most machine learning models focus on minimizing the prediction error, therby improving the prediction power done by a single model. However, boosting algorithms improve the quality of prediction by training a series of weak models on different subsets of the data, each compensating the weaknesses of its predecessors and then combining the predictions of these models to create a final prediction that is more accurate than the predictions of any individual model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:13.307164Z","iopub.status.busy":"2022-12-01T16:51:13.305456Z","iopub.status.idle":"2022-12-01T16:51:23.301507Z","shell.execute_reply":"2022-12-01T16:51:23.299783Z","shell.execute_reply.started":"2022-12-01T16:51:13.307099Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n","GB = GradientBoostingRegressor()\n","np.mean(cross_val_score(GB, X_train, y_train, cv=5))"]},{"cell_type":"markdown","metadata":{},"source":["Different regression models were evaluated based on the CV scores and itâs observed that Linear, Ridge,CatBoost,LGBM, SGD, Random Forest, Gradient Boosting regression best fits the data compared to all the other methods.  From cross validation it can be concluded that Haversine distance is the most prominant feature in describing the target variable. It is possible to try other functions of this feature to examine whether adding more features by feature transforming this variable brings about any improvement to the model. "]},{"cell_type":"markdown","metadata":{},"source":["**Feature transformation of Haversine distance**\n","\n","Feature transformation does change the distribution of the variable. When we have skewed distribution we use transformation to remove the skewness of the variable and hence changing the distribution. Functions like Log, square / cube, Square root / cube root, reciprocals can be used for feature transformation to reduce skewness of the variable. \n","\n","Log transformation, square root and other nth roots are used for removing right skewness from the data. On the other hand, left skewed data distribution can be transformed using nth power or exponents functions\n","\n","If we are taking the log of the variable that is negative or zero, it might show an error, as log of zero is undefined. Instead take log(x+c) where c is a constant with the objective that the log of the input must be greater than zero. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:23.306244Z","iopub.status.busy":"2022-12-01T16:51:23.305783Z","iopub.status.idle":"2022-12-01T16:51:25.946752Z","shell.execute_reply":"2022-12-01T16:51:25.945574Z","shell.execute_reply.started":"2022-12-01T16:51:23.30621Z"},"trusted":true},"outputs":[],"source":["df['haversine_distance_log'] = np.log(df['haversine_distance'].values + 1)\n","df['haversine_distance_sqrt'] = np.sqrt(df['haversine_distance'].values)\n","df['haversine_distance_sq'] = df['haversine_distance'].values**2\n","df_test['haversine_distance_log'] = np.log(df_test['haversine_distance'].values + 1)\n","df_test['haversine_distance_sqrt'] = np.sqrt(df_test['haversine_distance'].values)\n","f, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)#\n","sb.despine(left=True)\n","sb.distplot(df['haversine_distance'], label = 'haversine_distance',color=\"b\",bins = 100, ax=axes[0,0])\n","axes[0,0].set_title('Histogram of distance')\n","sb.distplot(df['haversine_distance_log'], label = 'haversine_distance_log',color=\"yellow\",bins =100, ax=axes[0,1])\n","axes[0,1].set_title('Histogram of log of distance')\n","sb.distplot(df['haversine_distance_sqrt'], label = 'haversine_distance_sqrt',color=\"magenta\",bins =100, ax=axes[1, 0])\n","axes[1,0].set_title('Histogram of sqrt of distance')\n","sb.distplot(df['haversine_distance_sq'], label = 'haversine_distance_sq',color=\"green\",bins =100, ax=axes[1, 1])\n","axes[1,1].set_title('Histogram of square of distance')\n","plt.setp(axes, yticks=[])\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["It is clearly observed how Log and Square root transformation reduced the skewness of the distribution, whereas square root increased the right skewness further. \n","\n","Including 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', back after  a lot of trial and error to get an improved score. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:25.959634Z","iopub.status.busy":"2022-12-01T16:51:25.958036Z","iopub.status.idle":"2022-12-01T16:51:25.983328Z","shell.execute_reply":"2022-12-01T16:51:25.981796Z","shell.execute_reply.started":"2022-12-01T16:51:25.959572Z"},"trusted":true},"outputs":[],"source":["df_train = df.drop(['key', 'pickup_datetime','pickup_weekday', 'fare', 'fare_amount', 'base_fare',\n","                    'haversine_distance', 'haversine_distance_sq', 'haversine_distance_sqrt'], axis = 1)\n","df_test_copy = df_test.drop(['key', 'base_fare', 'pickup_datetime','pickup_weekday', 'base_fare','haversine_distance', \n","                             'haversine_distance_sqrt'], axis = 1)\n","#df_test_copy = df_test.drop(['key', 'base_fare', 'pickup_datetime','pickup_weekday', 'base_fare','haversine_distance', 'haversine_distance_sqrt','pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude' ], axis = 1)\n","X = df_train.copy()\n","y = df['fare_amount']\n","df_train.columns, df_test_copy.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:25.986093Z","iopub.status.busy":"2022-12-01T16:51:25.985621Z","iopub.status.idle":"2022-12-01T16:51:27.13997Z","shell.execute_reply":"2022-12-01T16:51:27.138284Z","shell.execute_reply.started":"2022-12-01T16:51:25.986054Z"},"trusted":true},"outputs":[],"source":["scaler = preprocessing.StandardScaler()\n","X= scaler.fit(X).transform(X)\n","test_X= scaler.transform(df_test_copy)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","from sklearn.model_selection import cross_validate\n","cv_results = cross_validate(ridge, X_train, y_train, cv=5, return_estimator=True)\n","\n","Coefficient = []\n","for model in cv_results['estimator']:\n","    Coefficient.append(model.coef_)\n","\n","coefficient = pd.DataFrame(Coefficient, columns = df_train.columns)\n","abs(coefficient.mean(axis =0)).sort_values(ascending = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:27.143469Z","iopub.status.busy":"2022-12-01T16:51:27.14246Z","iopub.status.idle":"2022-12-01T16:51:27.464673Z","shell.execute_reply":"2022-12-01T16:51:27.46278Z","shell.execute_reply.started":"2022-12-01T16:51:27.14341Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import RidgeCV\n","ridge = RidgeCV(cv=5).fit(X_train, y_train)\n","ridge.score(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:27.467363Z","iopub.status.busy":"2022-12-01T16:51:27.466544Z","iopub.status.idle":"2022-12-01T16:51:27.551111Z","shell.execute_reply":"2022-12-01T16:51:27.549301Z","shell.execute_reply.started":"2022-12-01T16:51:27.467312Z"},"trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["## **<span style = 'color:green'>8. Model Evaluation & Kaggle Submission</span>**<a name ='Report'></a>\n","[<div style=\"text-align: right\"> Back to Table of contents</div>](#Table) \n","Define a function to evaluate the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:27.55441Z","iopub.status.busy":"2022-12-01T16:51:27.553201Z","iopub.status.idle":"2022-12-01T16:51:27.571209Z","shell.execute_reply":"2022-12-01T16:51:27.569825Z","shell.execute_reply.started":"2022-12-01T16:51:27.554354Z"},"trusted":true},"outputs":[],"source":["def model_train_evaluation(y, ypred, model_name): \n","       \n","    # Model Evaluation metrics\n","    from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, r2_score, mean_absolute_percentage_error\n","    print(\"\\n \\n Model Evaluation Report: \")\n","    print('Mean Absolute Error(MAE) of', model_name,':', mean_absolute_error(y, ypred))\n","    print('Mean Squared Error(MSE) of', model_name,':', mean_squared_error(y, ypred))\n","    print('Root Mean Squared Error (RMSE) of', model_name,':', mean_squared_error(y, ypred, squared = False))\n","    print('Mean absolute percentage error (MAPE) of', model_name,':', mean_absolute_percentage_error(y, ypred))\n","    print('Explained Variance Score (EVS) of', model_name,':', explained_variance_score(y, ypred))\n","    print('R2 of', model_name,':', (r2_score(y, ypred)).round(2))\n","    print('\\n \\n')\n","    \n","    # Actual vs Predicted Plot\n","    f, ax = plt.subplots(figsize=(12,6),dpi=100);\n","    plt.scatter(y, ypred, label=\"Actual vs Predicted\")\n","    # Perfect predictions\n","    plt.xlabel('Fare amount')\n","    plt.ylabel('Fare amount')\n","    plt.title('Expection vs Prediction')\n","    plt.plot(y,y,'r', label=\"Perfect Expected Prediction\")\n","    plt.legend()\n","    f.text(0.95, 0.06, 'AUTHOR: RINI CHRISTY',\n","         fontsize=12, color='green',\n","         ha='left', va='bottom', alpha=0.5);\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Linear regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:27.57754Z","iopub.status.busy":"2022-12-01T16:51:27.574373Z","iopub.status.idle":"2022-12-01T16:51:28.127209Z","shell.execute_reply":"2022-12-01T16:51:28.12605Z","shell.execute_reply.started":"2022-12-01T16:51:27.577452Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()\n","lr.fit (X_train, y_train)\n","Yhat_lr = lr.predict(X_test)\n","model_train_evaluation(y_test, Yhat_lr, 'Linear regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:28.129819Z","iopub.status.busy":"2022-12-01T16:51:28.128896Z","iopub.status.idle":"2022-12-01T16:51:28.169961Z","shell.execute_reply":"2022-12-01T16:51:28.166925Z","shell.execute_reply.started":"2022-12-01T16:51:28.129779Z"},"trusted":true},"outputs":[],"source":["test_pred = lr.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Ridge regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:28.175362Z","iopub.status.busy":"2022-12-01T16:51:28.173988Z","iopub.status.idle":"2022-12-01T16:51:28.646717Z","shell.execute_reply":"2022-12-01T16:51:28.645572Z","shell.execute_reply.started":"2022-12-01T16:51:28.175276Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","ridge = Ridge()\n","ridge.fit (X_train, y_train)\n","Yhat_ridge = ridge.predict(X_test)\n","model_train_evaluation(y_test, Yhat_ridge, 'Ridge regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:28.649346Z","iopub.status.busy":"2022-12-01T16:51:28.648638Z","iopub.status.idle":"2022-12-01T16:51:28.678686Z","shell.execute_reply":"2022-12-01T16:51:28.676782Z","shell.execute_reply.started":"2022-12-01T16:51:28.649306Z"},"trusted":true},"outputs":[],"source":["test_pred = ridge.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>Random Forest regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:28.682111Z","iopub.status.busy":"2022-12-01T16:51:28.681385Z","iopub.status.idle":"2022-12-01T16:51:41.467811Z","shell.execute_reply":"2022-12-01T16:51:41.466469Z","shell.execute_reply.started":"2022-12-01T16:51:28.682033Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","rf = RandomForestRegressor()\n","rf.fit (X_train, y_train)\n","Yhat_rf = rf.predict(X_test)\n","model_train_evaluation(y_test, Yhat_rf, 'Random Forest Regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:41.470204Z","iopub.status.busy":"2022-12-01T16:51:41.469792Z","iopub.status.idle":"2022-12-01T16:51:41.809881Z","shell.execute_reply":"2022-12-01T16:51:41.80866Z","shell.execute_reply.started":"2022-12-01T16:51:41.470166Z"},"trusted":true},"outputs":[],"source":["test_pred = rf.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>CatBoost regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:41.811806Z","iopub.status.busy":"2022-12-01T16:51:41.811389Z","iopub.status.idle":"2022-12-01T16:51:43.284848Z","shell.execute_reply":"2022-12-01T16:51:43.282967Z","shell.execute_reply.started":"2022-12-01T16:51:41.811772Z"},"trusted":true},"outputs":[],"source":["from catboost import CatBoostRegressor\n","Cat = CatBoostRegressor(loss_function='RMSE', learning_rate = 0.1, \n","                        max_depth = 5,  n_estimators = 100, silent = True)\n","Cat.fit (X_train, y_train)\n","Cat.fit (X_train, y_train)\n","Yhat_Cat = Cat.predict(X_test)\n","model_train_evaluation(y_test, Yhat_Cat, 'Ridge regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:43.288034Z","iopub.status.busy":"2022-12-01T16:51:43.28748Z","iopub.status.idle":"2022-12-01T16:51:43.309991Z","shell.execute_reply":"2022-12-01T16:51:43.308574Z","shell.execute_reply.started":"2022-12-01T16:51:43.287982Z"},"trusted":true},"outputs":[],"source":["test_pred = Cat.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>SGD regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:43.313445Z","iopub.status.busy":"2022-12-01T16:51:43.31253Z","iopub.status.idle":"2022-12-01T16:51:43.812913Z","shell.execute_reply":"2022-12-01T16:51:43.810891Z","shell.execute_reply.started":"2022-12-01T16:51:43.313401Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import SGDRegressor\n","SGD = SGDRegressor()\n","SGD.fit (X_train, y_train)\n","Yhat_SGD = SGD.predict(X_test)\n","model_train_evaluation(y_test, Yhat_SGD, 'SGD Regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:43.815671Z","iopub.status.busy":"2022-12-01T16:51:43.815217Z","iopub.status.idle":"2022-12-01T16:51:43.849399Z","shell.execute_reply":"2022-12-01T16:51:43.847692Z","shell.execute_reply.started":"2022-12-01T16:51:43.815635Z"},"trusted":true},"outputs":[],"source":["test_pred = SGD.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>LGBM regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:43.853521Z","iopub.status.busy":"2022-12-01T16:51:43.851711Z","iopub.status.idle":"2022-12-01T16:51:45.050786Z","shell.execute_reply":"2022-12-01T16:51:45.048924Z","shell.execute_reply.started":"2022-12-01T16:51:43.853438Z"},"trusted":true},"outputs":[],"source":["from lightgbm import LGBMRegressor\n","LGBM = LGBMRegressor (boosting_type = 'gbdt', num_leaves = 31,  learning_rate = 0.1, \n","                       max_depth = 5, n_estimators = 100, silent = True)\n","LGBM.fit (X_train, y_train)\n","Yhat_LGBM = LGBM.predict(X_test)\n","model_train_evaluation(y_test, Yhat_LGBM, 'LGBM Regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:45.053176Z","iopub.status.busy":"2022-12-01T16:51:45.052776Z","iopub.status.idle":"2022-12-01T16:51:45.102127Z","shell.execute_reply":"2022-12-01T16:51:45.100996Z","shell.execute_reply.started":"2022-12-01T16:51:45.053142Z"},"trusted":true},"outputs":[],"source":["test_pred = LGBM.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **<span style = 'color:brown'>GradientBoosting regression evaluation</span>**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:45.105161Z","iopub.status.busy":"2022-12-01T16:51:45.104208Z","iopub.status.idle":"2022-12-01T16:51:49.301789Z","shell.execute_reply":"2022-12-01T16:51:49.300305Z","shell.execute_reply.started":"2022-12-01T16:51:45.105108Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n","GB = GradientBoostingRegressor()\n","GB.fit (X_train, y_train)\n","Yhat_GB = GB.predict(X_test)\n","model_train_evaluation(y_test, Yhat_GB, 'Gradient Boosting Regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:49.303861Z","iopub.status.busy":"2022-12-01T16:51:49.303466Z","iopub.status.idle":"2022-12-01T16:51:49.33756Z","shell.execute_reply":"2022-12-01T16:51:49.335751Z","shell.execute_reply.started":"2022-12-01T16:51:49.303829Z"},"trusted":true},"outputs":[],"source":["test_pred = GB.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare_amount'])\n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:49.340409Z","iopub.status.busy":"2022-12-01T16:51:49.33935Z","iopub.status.idle":"2022-12-01T16:51:49.38127Z","shell.execute_reply":"2022-12-01T16:51:49.379582Z","shell.execute_reply.started":"2022-12-01T16:51:49.340363Z"},"trusted":true},"outputs":[],"source":["Submission.to_csv('Submission.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["**Using fare column instead of fare_amount column**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:49.383706Z","iopub.status.busy":"2022-12-01T16:51:49.383312Z","iopub.status.idle":"2022-12-01T16:51:49.415249Z","shell.execute_reply":"2022-12-01T16:51:49.413774Z","shell.execute_reply.started":"2022-12-01T16:51:49.383672Z"},"trusted":true},"outputs":[],"source":["df_test['haversine_distance_log'] = np.log(df_test['haversine_distance'].values + 1)\n","df_test['haversine_distance_sqrt'] = np.sqrt(df_test['haversine_distance'].values)\n","df_train = df.drop(['key', 'pickup_datetime','pickup_weekday', 'fare', 'fare_amount', 'base_fare', 'haversine_distance_sq', \n","                    'pickup_hour'], axis = 1)\n","df_test_copy = df_test.drop(['key', 'base_fare', 'pickup_datetime','pickup_weekday', 'pickup_hour'], axis = 1)\n","X = df_train.copy()\n","y = df['fare']\n","scaler = preprocessing.StandardScaler()\n","X= scaler.fit(X).transform(X)\n","test_X= scaler.transform(df_test_copy)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:49.419214Z","iopub.status.busy":"2022-12-01T16:51:49.417864Z","iopub.status.idle":"2022-12-01T16:51:50.083105Z","shell.execute_reply":"2022-12-01T16:51:50.082046Z","shell.execute_reply.started":"2022-12-01T16:51:49.419149Z"},"trusted":true},"outputs":[],"source":["from lightgbm import LGBMRegressor\n","LGBM = LGBMRegressor (boosting_type = 'gbdt', num_leaves = 31,  learning_rate = 0.1, \n","                       max_depth = 5, n_estimators = 100, silent = True)\n","LGBM.fit (X_train, y_train)\n","Yhat_LGBM = LGBM.predict(X_test)\n","model_train_evaluation(y_test, Yhat_LGBM, 'LGBM Regression Model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-01T16:51:50.085413Z","iopub.status.busy":"2022-12-01T16:51:50.084753Z","iopub.status.idle":"2022-12-01T16:51:50.130617Z","shell.execute_reply":"2022-12-01T16:51:50.12911Z","shell.execute_reply.started":"2022-12-01T16:51:50.085374Z"},"trusted":true},"outputs":[],"source":["test_pred = LGBM.predict(test_X)\n","Submission = pd.DataFrame(test_pred, columns = ['fare'])\n","Submission['fare_amount'] = df_test['base_fare'] + Submission['fare'] \n","Submission['key'] = df_test['key']\n","Submission = Submission[['key', 'fare_amount']]\n","Submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":61318,"sourceId":10170,"sourceType":"competition"}],"dockerImageVersionId":30301,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
